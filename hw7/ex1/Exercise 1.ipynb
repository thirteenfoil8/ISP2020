{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DBS_PATH = 'anon_data'\n",
    "\n",
    "def save_solution(file_path, movies):\n",
    "    with open(file_path, 'w') as f:\n",
    "        for movie in movies:\n",
    "            print(movie, file=f)\n",
    "            \n",
    "def load_oracle(file_path):\n",
    "    movies = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        movies = [movie.strip() for movie in f.readlines()]\n",
    "    return sorted(movies)\n",
    "\n",
    "def check_and_save_solution(solution, oracle_file_path, save_file_path):\n",
    "    solution = sorted(solution)\n",
    "    print('Movies rated by the target:', ', '.join(solution))\n",
    "    oracle = load_oracle(oracle_file_path)\n",
    "    print('\\n')\n",
    "    assert oracle == solution, 'Incorrect solution'\n",
    "    print('Solution found successfully!')\n",
    "    save_solution(save_file_path, solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Disclaimer: Some variables may have same names in different solutions and therefore make sure to execute cells of a single solution one by one without executing another cell!!!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_email = 'donald.trump@whitehouse.gov'\n",
    "\n",
    "local_db = pd.read_csv(f'{DBS_PATH}/com402-1.csv', header=None, names=['email_hash', 'movie_hash', 'date', 'rating'])\n",
    "public_db = pd.read_csv(f'{DBS_PATH}/imdb-1.csv', header=None, names=['email', 'movie', 'date', 'rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = pd.merge(local_db, public_db, how='inner', on=['date', 'rating'])\n",
    "\n",
    "movie_list = correlations['movie'].unique()\n",
    "movie_hashes = {}\n",
    "\n",
    "for movie in movie_list:\n",
    "    possible_hashes = correlations.loc[correlations['movie'] == movie]['movie_hash'].unique().tolist()\n",
    "    best_hash, count = '', 0\n",
    "    for possible_movie_hash in possible_hashes:\n",
    "        current_count = correlations.loc[\n",
    "                (correlations['movie'] == movie) & (correlations['movie_hash'] == possible_movie_hash)].size\n",
    "        if current_count > count:\n",
    "            count = current_count\n",
    "            best_hash = possible_movie_hash\n",
    "    movie_hashes[movie] = best_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies rated by the target: 12 Angry Men, Modern Times, North by Northwest, Once Upon a Time in the West, Pulp Fiction, Raiders of the Lost Ark, Seven Samurai, The Dark Knight, The Shawshank Redemption, Tokyo Story\n",
      "\n",
      "\n",
      "Solution found successfully!\n"
     ]
    }
   ],
   "source": [
    "# Get ratings rated by the given user in the public database\n",
    "target_public_ratings = public_db.loc[public_db['email'] == target_email]\n",
    "# Find entries on same dates and with same ratings on local database\n",
    "target_hash_arr = \\\n",
    "    pd.merge(local_db, target_public_ratings, how='inner', on=['date', 'rating'])['email_hash'].unique()\n",
    "\n",
    "# ensure there is only 1 user in the given set and pick his email hash\n",
    "assert len(target_hash_arr) == 1\n",
    "target_hash = target_hash_arr[0]\n",
    "\n",
    "rated_movies = local_db.loc[local_db['email_hash'] == target_hash]['movie_hash'].unique().tolist()\n",
    "reversed_movie_hashes = {movie_hashes[k]: k for k in movie_hashes}\n",
    "\n",
    "\n",
    "rated_movies_cleartext = [reversed_movie_hashes[movie_hash] for movie_hash in rated_movies]\n",
    "\n",
    "\n",
    "check_and_save_solution(rated_movies_cleartext, 'real_data/user-1.csv', 'real_data/solution-1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_db = pd.read_csv(f'{DBS_PATH}/com402-2.csv', header=None, names=['email_hash', 'movie_hash', 'date', 'rating'])\n",
    "public_db = pd.read_csv(f'{DBS_PATH}/imdb-2.csv', header=None, names=['email', 'movie', 'date', 'rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(col):\n",
    "    return (col - col.mean()) / col.std()\n",
    "\n",
    "def minmax_normalize(col):\n",
    "    return (col - col.min()) / (col.max() - col.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_movie_freq = local_db.groupby(['movie_hash']).size().reset_index(name='freq').sort_values(by='freq')\n",
    "local_movie_freq['freq'] = normalize(local_movie_freq['freq'])\n",
    "\n",
    "public_movie_freq = public_db.groupby(['movie']).size().reset_index(name='freq').sort_values(by='freq')\n",
    "public_movie_freq['freq'] = normalize(public_movie_freq['freq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_hash_correlation = list(zip(list(local_movie_freq['movie_hash'].unique()), list(public_movie_freq['movie'].unique())))\n",
    "movie_to_hash_mapping = { k: v for v, k in movie_hash_correlation }\n",
    "hash_to_movie_mapping = { k: v for k, v in movie_hash_correlation }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_EMAIL = 'donald.trump@whitehouse.gov'\n",
    "movies_rated_by_target = public_db.loc[public_db['email'] == TARGET_EMAIL]['movie'].unique()\n",
    "\n",
    "rated_movie_hashes = set([movie_to_hash_mapping[m] for m in movies_rated_by_target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies rated by the target: 12 Angry Men, Casablanca, Citizen Kane, Double Indemnity, Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb, Lawrence of Arabia, Modern Times, Psycho, Raiders of the Lost Ark, Rashomon, Schindler's List, Seven Samurai, Singin' in the Rain, Spirited Away, Star Wars: Episode V - The Empire Strikes Back, Sunset Blvd., Taxi Driver, The Dark Knight, The Godfather, The Godfather: Part II\n",
      "\n",
      "\n",
      "Solution found successfully!\n"
     ]
    }
   ],
   "source": [
    "email_hashes = local_db['email_hash'].unique()\n",
    "candidates = []\n",
    "for email_hash in email_hashes:\n",
    "    rated_movies_by_email_hash = set(local_db.loc[local_db['email_hash'] == email_hash]['movie_hash'].unique())\n",
    "    if rated_movie_hashes.issubset(rated_movies_by_email_hash):\n",
    "        candidates.append(email_hash)\n",
    "assert len(candidates) == 1, 'No unique candidate or none at all'\n",
    "target_email_hash = candidates[0]\n",
    "\n",
    "local_rated_movies_hashes = local_db.loc[local_db['email_hash'] == target_email_hash]['movie_hash'].unique()\n",
    "local_rated_movies = [hash_to_movie_mapping[mh] for mh in local_rated_movies_hashes]\n",
    "rated_movies_cleartext = sorted(local_rated_movies)\n",
    "\n",
    "check_and_save_solution(rated_movies_cleartext, 'real_data/user-2.csv', 'real_data/solution-2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_EMAIL = 'donald.trump@whitehouse.gov'\n",
    "local_db = pd.read_csv(f'{DBS_PATH}/com402-3.csv', header=None, names=['email_hash', 'movie_hash', 'date', 'rating'])\n",
    "public_db = pd.read_csv(f'{DBS_PATH}/imdb-3.csv', header=None, names=['email', 'movie', 'date', 'rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import collections\n",
    "\n",
    "# class Multiset(object):\n",
    "#     def __init__(self, iterable):\n",
    "#         self.counter = collections.Counter(iterable)\n",
    "        \n",
    "#     def __repr__(self):\n",
    "#         return str(self.counter)\n",
    "    \n",
    "#     def issubmultiset(self, other):\n",
    "#         for k in self.counter.keys():\n",
    "#             if k not in other.counter or self.counter[k] > other.counter[k]:\n",
    "#                 return False\n",
    "#         return True\n",
    "\n",
    "emails = public_db['email'].unique()\n",
    "# email_ratings = {\n",
    "#     email: Multiset(list(public_db.loc[public_db['email'] == email]['rating']))\\\n",
    "#     for email in emails\n",
    "# }\n",
    "\n",
    "email_hashes = local_db['email_hash'].unique()\n",
    "# email_hash_ratings = {\n",
    "#     email_hash: Multiset(list(local_db.loc[local_db['email_hash'] == email_hash]['rating']))\\\n",
    "#     for email_hash in email_hashes\n",
    "# }\n",
    "\n",
    "# Ineffective for mapping mails to hashes as there are around 100 to 150 possible solutions for each\n",
    "# email this way\n",
    "#for e, r in email_ratings.items():\n",
    "#    k = 0\n",
    "#    for eh, hr in email_hash_ratings.items():\n",
    "#        if r.issubmultiset(hr):\n",
    "#            k += 1\n",
    "#    print(k)\n",
    "#list(public_db.loc[public_db['email'] == TARGET_EMAIL]['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "\n",
    "# d1 = datetime.strptime('31/03/15', '%d/%m/%y')\n",
    "# d2 = datetime.strptime('25/03/15', '%d/%m/%y')\n",
    "# (d2 - d1).days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing datetime.\n",
    "from datetime import datetime\n",
    "\n",
    "def date_comparator(d1, d2):\n",
    "    return (d1 - d2).days\n",
    "\n",
    "class ConditionedDateSet:\n",
    "    def __init__(self, entries, comparator):\n",
    "        self.data = sorted([datetime.strptime(entry, '%d/%m/%y') for entry in entries])\n",
    "        self.comparator = comparator\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.data)\n",
    "        \n",
    "    def check_overlaps_left(self, other):\n",
    "        MAX_POSSIBLE_DISTANCE = 57 # might be +- 1 off\n",
    "        k = 0\n",
    "        result = []\n",
    "        max_len = len(other.data)\n",
    "        for i, v in enumerate(self.data):\n",
    "            while k < max_len and self.comparator(v, other.data[k]) > MAX_POSSIBLE_DISTANCE:\n",
    "                k += 1\n",
    "            current = k\n",
    "            overlaps = []\n",
    "            while current < max_len and self.comparator(other.data[current], v) <= MAX_POSSIBLE_DISTANCE:\n",
    "                overlaps.append(other.data[current])\n",
    "                current += 1\n",
    "            if not overlaps:\n",
    "                #print(current, k, other.data[current], v, self.comparator(other.data[current], v))\n",
    "                return None\n",
    "            result.append(overlaps)\n",
    "        return result\n",
    "\n",
    "    \n",
    "email_dates = {\n",
    "    email: ConditionedDateSet(\n",
    "        public_db.loc[public_db['email'] == email]['date'],\n",
    "        date_comparator)\\\n",
    "    for email in emails\n",
    "}\n",
    "\n",
    "email_hashes_dates = {\n",
    "    email_hash: ConditionedDateSet(\n",
    "        local_db.loc[local_db['email_hash'] == email_hash]['date'],\n",
    "        date_comparator)\\\n",
    "    for email_hash in email_hashes\n",
    "}\n",
    "\n",
    "pairs = []\n",
    "\n",
    "while len(email_hashes_dates) > 0:\n",
    "    found = []\n",
    "    for email, cdset in email_dates.items():\n",
    "        matches = []\n",
    "        for email_hash, hashes_cdset in email_hashes_dates.items():\n",
    "            if cdset.check_overlaps_left(hashes_cdset) is not None:\n",
    "                matches.append(email_hash)\n",
    "        if len(matches) == 0:\n",
    "            print('Oh no!')\n",
    "            raise ValueError('Algorithm failed')\n",
    "        elif len(matches) == 1:\n",
    "            del email_hashes_dates[matches[0]]\n",
    "            found.append(email)\n",
    "            pairs.append((email, matches[0]))\n",
    "    for email in found:\n",
    "        del email_dates[email]\n",
    "\n",
    "email_to_hash_mapping = { k: v for k, v in pairs }\n",
    "hash_to_email_mapping = { k: v for v, k in pairs }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_raters = {\n",
    "    movie: set(public_db.loc[public_db['movie'] == movie]['email'].unique())\\\n",
    "    for movie in public_db['movie'].unique()\n",
    "}\n",
    "\n",
    "hash_movie_raters = {\n",
    "    hash_movie: set([ hash_to_email_mapping[email_hash] for email_hash in local_db.loc[local_db['movie_hash'] == hash_movie]['email_hash'].unique()])\\\n",
    "    for hash_movie in local_db['movie_hash'].unique()\n",
    "}\n",
    "\n",
    "\n",
    "movie_hash_pairs = []\n",
    "while len(hash_movie_raters) > 0:\n",
    "    found = []\n",
    "    for movie, mv in movie_raters.items():\n",
    "        matches = []\n",
    "        for movie_hash, mhv in hash_movie_raters.items():\n",
    "            if mv.issubset(mhv):\n",
    "                matches.append(movie_hash)\n",
    "        assert len(matches) > 0, 'Algorithm faulty'\n",
    "        if len(matches) == 1:\n",
    "            found.append(movie)\n",
    "            movie_hash_pairs.append((movie, matches[0]))\n",
    "            del hash_movie_raters[matches[0]]\n",
    "    for movie in found:\n",
    "        del movie_raters[movie]\n",
    "        \n",
    "movie_to_hash_mapping = { k: v for k, v in movie_hash_pairs }\n",
    "hash_to_movie_mapping = { k: v for v, k in movie_hash_pairs }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies rated by the target: 12 Angry Men, A Clockwork Orange, A Hard Day's Night, A Man Escaped, A Separation, A Streetcar Named Desire, Amadeus, Amelie, American Beauty, Amores perros, Anatomy of a Murder, Before Sunrise, Being There, Bicycle Thieves, Blade Runner, Blue Velvet, Boyhood, Chungking Express, Cinema Paradiso, Citizen Kane, City Lights, City of God, Die Hard, Do the Right Thing, Double Indemnity, Fight Club, Finding Nemo, Frankenstein, Full Metal Jacket, Gone with the Wind, Goodfellas, Grave of the Fireflies, Hannah and Her Sisters, Harakiri, Harold and Maude, Heat, High Noon, Howl's Moving Castle, Ikiru, In the Mood for Love, Indiana Jones and the Last Crusade, La Dolce Vita, La Grande Illusion, La Haine, Laura, Le Samoura, Life of Brian, Los Olvidados, M, Memento, Metropolis, Modern Times, North by Northwest, Notorious, On the Waterfront, Persepolis, Persona, Princess Mononoke, Psycho, Rear Window, Rebecca, Repulsion, Reservoir Dogs, Rififi, Roman Holiday, Rope, Schindler's List, Se7en, Seven Samurai, Stalker, Sullivan's Travels, The Adventures of Robin Hood, The Big Lebowski, The Big Sleep, The Bridge on the River Kwai, The Conversation, The Deer Hunter, The Departed, The French Connection, The Good, the Bad and the Ugly, The Hunt, The Killing, The Lives of Others, The Maltese Falcon, The Mirror, The Secret in Their Eyes, The Social Network, The Sting, The Straight Story, The Treasure of the Sierra Madre, The Wild Bunch, The Wizard of Oz, Three Colors: Red, Throne of Blood, To Be or Not to Be, Ugetsu monogatari, Vertigo, Viridiana, Wild Strawberries, Wings of Desire\n",
      "\n",
      "\n",
      "Solution found successfully!\n"
     ]
    }
   ],
   "source": [
    "movies_hashes_rated_by_the_target = \\\n",
    "    local_db.loc[local_db['email_hash'] == email_to_hash_mapping[TARGET_EMAIL]]['movie_hash'].unique()\n",
    "movies_rated_by_the_target = sorted([\n",
    "    hash_to_movie_mapping[movie_hash]\\\n",
    "    for movie_hash in movies_hashes_rated_by_the_target\n",
    "])\n",
    "\n",
    "check_and_save_solution(movies_rated_by_the_target,'real_data/user-3.csv', 'real_data/solution-3.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
